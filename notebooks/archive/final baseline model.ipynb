{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52222325-9edd-4866-96fe-c55270c97bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pydot\n",
    "import graphviz\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB0\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import AveragePooling2D, GlobalAveragePooling2D, Dense, Input\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Add\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11fbad5e-d31d-44a7-b90d-d2f45be7da16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydot in /Users/zhenliu15471/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /Users/zhenliu15471/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages (from pydot) (3.0.6)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f81330b-ef19-426c-9426-84b334ce059a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in /Users/zhenliu15471/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages (0.20)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d2f97b-dd67-4570-aa0c-dd0b0c75dbfa",
   "metadata": {},
   "source": [
    "# Get the X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90c00af4-9907-404c-9d71-712bd69f6f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b93612-94eb-4d08-b33c-999ee1ba4710",
   "metadata": {},
   "source": [
    "## Load data for X1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a54385-b174-4691-b3ed-18efc98fe2c8",
   "metadata": {},
   "source": [
    "### Import features file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a9bfb9a-3f64-4677-a524-0e5fd6f34737",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_file = '/Users/zhenliu15471/code/CMaxK/robo_romeo/raw_data/encoded_images_6k.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "582121bf-0e42-4786-96de-974fbaaa3dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(features_file, 'rb')\n",
    "features_dict = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21958257-4cd8-4f48-a07e-7c7163b5ca4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 8, 1280)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_dict['2513260012_03d33305cf'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1de1cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bbb08b1-9710-4745-b12c-8841853c4a23",
   "metadata": {},
   "source": [
    "## Create the X1_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2584c167-782c-4924-b263-e169adee014d",
   "metadata": {},
   "source": [
    "### Import captions data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45c3565e-6aab-4368-b44e-260720966f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_file = '/Users/zhenliu15471/code/CMaxK/robo_romeo/raw_data/cap'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "edfa6803-bc5b-4270-8a89-16b09d28cfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(captions_file, 'rb')\n",
    "x2_captions = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa458361-2528-4faa-b2cc-04cd39b3b527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1000268201_693b08cb0e'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap_img_list = x2_captions[0] # first column with list of repeated image id's\n",
    "cap_img_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9ddc15-d269-46ed-abda-027e017fe828",
   "metadata": {},
   "source": [
    "### Create X1_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f785fcb-966b-46d6-8de2-dd4ca4717131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 8, 8, 1280)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1_train = []\n",
    "for cap_img in cap_img_list[:10000]:\n",
    "    img_feature_matrix = features_dict[cap_img][0]\n",
    "    X1_train.append(img_feature_matrix)\n",
    "X1_train = np.array(X1_train)\n",
    "X1_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d6d5e2-2a45-48ef-a20d-f827325cac7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffffr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b1c153-2693-4d51-8dee-300343781456",
   "metadata": {},
   "source": [
    "## Create X2_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ad00049-987e-4dd7-9686-61ed7a0f6f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 36)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2_train = np.array(x2_captions[1]).astype(np.uint32)[:10000]\n",
    "X2_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "876fc660-f5c5-4ecf-97d2-5f2d6be19aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2_captions[2][13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24757bcb-3861-4663-add6-22799703079f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2,   1,  34,   3,   1,  72, 129,   5, 121,  57,   1, 473,  12,\n",
       "       624,   3,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=uint32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2_train[14]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f5c826-e0c8-4c18-bd51-7c03640a53c7",
   "metadata": {},
   "source": [
    "## Create y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9acdeede-181f-4908-bb6d-bb28548b571c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 7589"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d50453b7-8de0-48a7-8a93-210098513491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,  34,   3, ...,  11,  93, 211])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.array([el[0] if len(el)>0 else vocab_size for el in x2_captions[2][:10000]])\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62f37ca2-6131-426e-956b-4e716e0f296c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x2_captions=None\n",
    "del x2_captions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e2c5e2-af12-46f2-85c3-8a2d839e728c",
   "metadata": {},
   "source": [
    "#  LSTM sequence model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7bcb6c-d2fa-49c9-ba94-d55be4a5956c",
   "metadata": {},
   "source": [
    "## LSTM Model layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62bec15c-562f-428c-b220-0554940a11e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_caption_length = 36\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ace54d92-1f9b-4d6d-95fc-59038b208f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 16:17:37.005008: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "inputs2  = Input(shape=(max_caption_length,),name=\"captions\")\n",
    "embed_layer = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "\n",
    "input_encoded = Input(shape=(8,8,1280),name=\"images_encoded\")\n",
    "\n",
    "pooling = GlobalAveragePooling2D()(input_encoded)\n",
    "\n",
    "cnn_dense = Dense(256, activation='relu')(pooling)\n",
    "\n",
    "\n",
    "combine = Add()([embed_layer,cnn_dense])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "daae1b71-946c-4931-b216-fe3d4542b9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lstm_layer = LSTM(256)(combine)\n",
    "decoder = Dense(1000, activation='relu')(lstm_layer)\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d140bef-f0bf-49ce-9496-dead3a2ea4ce",
   "metadata": {},
   "source": [
    "##### Model summary Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "06a09af6-da5e-4f0f-a856-344962929586",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Model(inputs=[input_encoded, inputs2], outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy' , optimizer='adam',\n",
    "             metrics = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "daa5cf19-5eb3-44c7-9d29-516aa8b9cc03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "images_encoded (InputLayer)     [(None, 8, 8, 1280)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "captions (InputLayer)           [(None, 36)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 1280)         0           images_encoded[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 36, 256)      1942784     captions[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          327936      global_average_pooling2d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 36, 256)      0           embedding[0][0]                  \n",
      "                                                                 dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 256)          525312      add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1000)         257000      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 7589)         7596589     dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 10,649,621\n",
      "Trainable params: 10,649,621\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb7aeb0f-2505-49d6-bcf8-799a198f31f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "plot_model(model,show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e27c13-835c-4cfa-bf4b-ba1ccf0b0edb",
   "metadata": {},
   "source": [
    "# Fit and train model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7e92206b-155a-48e7-9f6f-8f30aa853a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "10dd7cd0-6b57-460a-869c-a0c89d1449b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=7589)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "92f5cc5d-4f7c-42f8-b595-b1fd114d2228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 7589)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c0320717-4fc1-4df6-9860-801b0248b909",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 16:18:00.679005: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "313/313 [==============================] - 23s 68ms/step - loss: 5.3570 - accuracy: 0.1509\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 22s 70ms/step - loss: 4.3783 - accuracy: 0.2349\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 21s 68ms/step - loss: 3.7784 - accuracy: 0.2858\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 21s 68ms/step - loss: 3.3168 - accuracy: 0.3200\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 21s 68ms/step - loss: 2.9282 - accuracy: 0.3659\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 21s 68ms/step - loss: 2.5816 - accuracy: 0.4038\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 21s 68ms/step - loss: 2.2485 - accuracy: 0.4378\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 21s 68ms/step - loss: 1.9496 - accuracy: 0.4778\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 22s 69ms/step - loss: 1.7007 - accuracy: 0.5174\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 21s 68ms/step - loss: 1.4767 - accuracy: 0.5561\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1949bc370>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The fit\n",
    "model.fit(\n",
    "x=(X1_train,X2_train),\n",
    "    y=y_train,\n",
    "    batch_size=32,\n",
    "    epochs=10, \n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12f2de3-7bf9-4b8d-adeb-2a1f8a6025a6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8121d31e-f5f4-4106-b36d-ffe9e51f4bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa5b3670-e040-412b-9587-812561d2b643",
   "metadata": {},
   "source": [
    "# Predict the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5c8a73ea-b75e-452e-af36-161a1bbf1833",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_model = EfficientNetB0(\n",
    "    include_top=False, # Whether to include the fully-connected layer at the top of the network\n",
    "    weights='imagenet', # pre-trained weights on ImageNet\n",
    "    input_tensor=None,\n",
    "    input_shape= (256,256,3), # It should have exactly 3 inputs channels\n",
    "    pooling=None # Optional pooling mode for feature extraction when include_top is False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "69b92d2a-17bb-40f7-9f24-9c949aae0749",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_path = \"../raw_data/Flicker8k_Dataset/2718495608_d8533e3ac5.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "858e0bc3-176a-48bf-8563-25f225e21ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img= image.load_img(test_img_path, target_size=(256,256,3))\n",
    "x_test = image.img_to_array(test_img)\n",
    "x_test = np.expand_dims(x_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9b1fabca-a3a9-42d5-aa72-6b00675066aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = CNN_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5fce059e-d188-43de-a751-48f50a1be708",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "inputs_seq='startsequence'\n",
    "img_encoded=test\n",
    "inputs_seq = [2]\n",
    "# seq = \n",
    "for i in range(36):\n",
    "    inputs_seq_model = pad_sequences([inputs_seq],padding='post',maxlen=36)\n",
    "    y_pre = model.predict([img_encoded,inputs_seq_model])\n",
    "    next_word = y_pre.argmax()\n",
    "    if next_word == 13:\n",
    "        break\n",
    "    inputs_seq.append(next_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5a5f8ef6-4016-4101-a9de-f06b2da9fb02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 21, 177, 16, 3, 1, 1238, 3, 52, 12, 1, 442]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5fa9c7",
   "metadata": {},
   "source": [
    "# Transfer the token to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b6230eaf-c90c-42b1-98b3-711f536b8649",
   "metadata": {},
   "outputs": [],
   "source": [
    "file ='../raw_data/word_index'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1a8bb980-443a-4944-97fe-3f39ccd57e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file,'rb') as dictionary:\n",
    "    b = pickle.load(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ab4325fd-6855-403c-94a6-2120305631e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence  = []\n",
    "\n",
    "for number in inputs_seq:\n",
    "\n",
    "    sentence.append(list(b.keys())[list(b.values()).index(number)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "21094aad-61eb-4b21-93c5-4cafbe612db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = sentence[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1704913c-65c5-47ce-b4d7-10ef7ae443ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sentence  = ' '.join(word for word in sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a3a5e542-73ae-4f24-81d9-c4b0ca6e8088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a young asian boy in a pacifier in front of a railing'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8129a7f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m93"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
