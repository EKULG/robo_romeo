{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b59b4492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pydot\n",
    "import graphviz\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB0\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import AveragePooling2D, GlobalAveragePooling2D, Dense, Input\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3810011f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydot\n",
      "  Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /Users/theebak/.pyenv/versions/3.8.12/envs/robo_romeo/lib/python3.8/site-packages (from pydot) (3.0.9)\n",
      "Installing collected packages: pydot\n",
      "Successfully installed pydot-1.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "25b59b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting graphviz\n",
      "  Using cached graphviz-0.20-py3-none-any.whl (46 kB)\n",
      "Installing collected packages: graphviz\n",
      "Successfully installed graphviz-0.20\n"
     ]
    }
   ],
   "source": [
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8917ef",
   "metadata": {},
   "source": [
    "## Build CNN model with Pooling and Dense layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a879ca",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test on one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f3016ca9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "img_path = '../raw_data/images/10815824_2997e03d76.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1334a4cb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "img = image.load_img(img_path, target_size=(256,256,3))\n",
    "x = image.img_to_array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ee036de7",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 256, 256, 3)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.expand_dims(x, axis=0)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa52a44",
   "metadata": {},
   "source": [
    "### CNN Model layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a0182e",
   "metadata": {},
   "source": [
    "# Main notebook for data processing in robo_romeo project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0614dc0a",
   "metadata": {},
   "source": [
    "## Imports - this should do us for the whole project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8dbd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ChrisKarg/.pyenv/versions/lewagon/lib/python3.8/site-packages/pandas/compat/__init__.py:124: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n",
      "/var/folders/zb/g5x4lcv10b16pnz1jhn6gx340000gn/T/ipykernel_79373/3936168975.py:20: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  tqdm().pandas()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe30858024d54416aa747f3269cb39f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import string\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.xception import Xception #to get pre-trained model Xception\n",
    "from tensorflow.keras.applications.xception import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer #for text tokenization\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import add\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense#Keras to build our CNN and LSTM\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dropout\n",
    "from tqdm import tqdm_notebook as tqdm #to check loop progress\n",
    "tqdm().pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32220b64",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c757e1",
   "metadata": {},
   "source": [
    " - load_doc( filename ) – To load the document file and read the contents of the file into a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a62943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the document file into memory\n",
    "def load_doc(filename):\n",
    "    # Open file to read\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f124dab6",
   "metadata": {},
   "source": [
    " - load_descriptions(doc) – To create a description dictionary that will map images with all 5 captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d1071a",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "# extract descriptions for images\n",
    "def load_descriptions(doc):\n",
    "    mapping = dict()\n",
    "    # process lines\n",
    "    for line in doc.split('\\n'):\n",
    "        # split line by white space\n",
    "        tokens = line.split()\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        # take the first token as the image id, the rest as the description\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        # remove filename from image id\n",
    "        image_id = image_id.split('.')[0]\n",
    "        # convert description tokens back to string\n",
    "        image_desc = ' '.join(image_desc)\n",
    "        # create the list if needed\n",
    "        if image_id not in mapping:\n",
    "            mapping[image_id] = list()\n",
    "        # store description\n",
    "        mapping[image_id].append(image_desc)\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad247809",
   "metadata": {},
   "source": [
    " - clean_descriptions( descriptions) – to clean the data by taking all descriptions as input. This will perform several types of cleaning including uppercase to lowercase conversion, punctuation removal, and removal of the number containing words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0372a892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean descriptions\n",
    "clean_descriptions(descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0b5488",
   "metadata": {},
   "source": [
    " - txt_vocab( descriptions ) – to create a vocabulary from all the unique words extracted out from descriptions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5b34bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the loaded descriptions into a vocabulary of words\n",
    "def to_vocabulary(descriptions):\n",
    "    # build a list of all description strings\n",
    "    all_desc = set()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.update(d.split()) for d in descriptions[key]]\n",
    "    return all_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2d8d34",
   "metadata": {},
   "source": [
    " - save_descriptions( descriptions, filename ) – This function is used to store all the preprocessed descriptions into a file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236707c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e15d507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save descriptions to file, one per line\n",
    "def save_descriptions(descriptions, filename):\n",
    "    lines = list()\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for desc in desc_list:\n",
    "            lines.append(key + ' ' + desc)\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ddcca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Flickr8k_text/Flickr8k.token.txt'\n",
    "# load descriptions\n",
    "doc = load_doc(filename)\n",
    "# parse descriptions\n",
    "descriptions = load_descriptions(doc)\n",
    "print('Loaded: %d ' % len(descriptions))\n",
    "# clean descriptions\n",
    "clean_descriptions(descriptions)\n",
    "# summarize vocabulary\n",
    "vocabulary = to_vocabulary(descriptions)\n",
    "print('Vocabulary Size: %d' % len(vocabulary))\n",
    "# save to file\n",
    "save_descriptions(descriptions, 'descriptions.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86509339",
   "metadata": {},
   "source": [
    "# Extract feature vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ed60cd",
   "metadata": {},
   "source": [
    "Xception model takes images in size 299x299x3. we need to edit the last calssification layer.\n",
    "Extract_features() function extracts features for all images and puts the features dictionary into a pickle file named “features.p”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a335982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(directory):\n",
    "    model = Xception( include_top=False, pooling='avg' )\n",
    "    features = {}\n",
    "    for pic in tqdm(os.listdir(dir)):\n",
    "        file = dir + \"/\" + pic\n",
    "        image = Image.open(file)\n",
    "        image = image.resize((299,299))\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        image = image/127.5\n",
    "        image = image - 1.0\n",
    "        feature = model.predict(image)\n",
    "        features[image] = feature\n",
    "    return features\n",
    "#2048 feature vector\n",
    "features = extract_features(dataset_images)\n",
    "dump(features, open(\"features.p\",\"wb\"))\n",
    "#to directly load the features from the pickle file.\n",
    "features = load(open(\"features.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973b084d",
   "metadata": {},
   "source": [
    "# loading dataset for model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5296709",
   "metadata": {},
   "source": [
    "Flickr_8k.trainImages.txt contains a list of 6000 image names that are used for training\n",
    "\n",
    "Functions required to load the training datasets:\n",
    "\n",
    " - load_photos( fname ) – takes a file name as a parameter and return the list of image names by loading the text file into a string.\n",
    "\n",
    " - load_clean_descriptions( fname, image) – stores the captions for every image from the list of photos to a dictionary. For the ease of the LSTM model in identifying the beginning and ending of a caption, we append the and identifier with each caption. ('start' and 'end' tags at the beginning and end of each caption.\n",
    "\n",
    " - load_features(photos) – to return the extracted feature vectors from the Xception model and the dictionary for photos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c004e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "def load_photos(filename):\n",
    "    file = load_doc(filename)\n",
    "    photos = file.split(\"n\")[:-1]\n",
    "    return photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214562d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_descriptions(filename, photos):\n",
    "    #loading clean_descriptions\n",
    "    file = load_doc(filename)\n",
    "    descriptions = {}\n",
    "    for line in file.split(\"n\"):\n",
    "        words = line.split()\n",
    "        if len(words)<1 :\n",
    "            continue\n",
    "        image, image_caption = words[0], words[1:]\n",
    "        if image in photos:\n",
    "            if image not in descriptions:\n",
    "                descriptions[image] = []\n",
    "                desc = ' ' + \" \".join(image_caption) + ' '\n",
    "                descriptions[image].append(desc)\n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276fdc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features(photos):\n",
    "    #loading all features\n",
    "    all_features = load(open(\"features.p\",\"rb\"))\n",
    "    #selecting only needed features\n",
    "    features = {k:all_features[k] for k in photos}\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a497833a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = dataset_text + \"/\" + \"Flickr_8k.trainImages.txt\"\n",
    "#train = loading_data(filename)\n",
    "train_imgs = load_photos(filename)\n",
    "train_descriptions = load_clean_descriptions(\"descriptions.txt\", train_imgs)\n",
    "train_features = load_features(train_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad73146",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b4b799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff24e697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5184860",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c8569c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5752e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e652267",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs1 = Input(shape=(256,256,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c1ecf7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_model = EfficientNetB0(\n",
    "    include_top=False, # Whether to include the fully-connected layer at the top of the network\n",
    "    weights='imagenet', # pre-trained weights on ImageNet\n",
    "    input_tensor=None,\n",
    "    input_shape= (256,256,3), # It should have exactly 3 inputs channels\n",
    "    pooling=None # Optional pooling mode for feature extraction when include_top is False\n",
    ")(inputs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "108acba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooling = GlobalAveragePooling2D()(CNN_model)\n",
    "cnn_dense = Dense(256, activation='relu')(pooling)\n",
    "model1 = Model(inputs=inputs1, outputs=cnn_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3aaa1a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 256, 256, 3)]     0         \n",
      "                                                                 \n",
      " efficientnetb0 (Functional)  (None, 8, 8, 1280)       4049571   \n",
      "                                                                 \n",
      " global_average_pooling2d_5   (None, 1280)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               327936    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,377,507\n",
      "Trainable params: 4,335,484\n",
      "Non-trainable params: 42,023\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63150b20",
   "metadata": {},
   "source": [
    "## Combine with LSTM sequence model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54149cd",
   "metadata": {},
   "source": [
    "### LSTM Model layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a1956542",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_caption_length = 32+1\n",
    "vocab_size = 8763+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "54f746fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs2 = Input(shape=(max_caption_length,))\n",
    "embed_layer = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "lstm_layer = LSTM(256)(embed_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c514f0d2",
   "metadata": {},
   "source": [
    "### Combine CNN and LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bbd56246",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder1 = Add()([cnn_dense,lstm_layer])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746a22a5",
   "metadata": {},
   "source": [
    "### Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5178432b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f6cac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "09c889f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "plot_model(model,show_shapes=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
